#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import requests
import pandas as pd
from tqdm import tqdm
import re
import os
import warnings
warnings.filterwarnings("ignore")
import time
import zipfile
import pymol2
import sys
import matplotlib.pyplot as plt
import xml.etree.ElementTree as ET
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLineEdit, QFileDialog, QLabel,QMessageBox
from PyQt5.QtGui import QFont

def show_message(message):
    msg = QMessageBox()
    msg.setWindowTitle("X-Span  Version 1.0.0")
    msg.setText(message)
    msg.setStandardButtons(QMessageBox.Ok)
    msg.setStyleSheet("QLabel { font-family: 'Courier New'; font-size: 12pt; }")
    msg.setFixedSize(1600, 1600)
    msg.show()
    return msg 

def GUI():
    class FileNameGenerator(QWidget):
        def __init__(self):
            super().__init__()
            self.initUI()
        
        def initUI(self):
            self.resize(400, 200)
            layout = QVBoxLayout()
            ################################### input file path
            self.fpath=QLabel("Input file path:",self)
            layout.addWidget(self.fpath)   
            self.file_path = QLineEdit(self)
            self.file_path.setPlaceholderText("Select input file")
            layout.addWidget(self.file_path)
            self.browse_button = QPushButton("Browse", self)
            self.browse_button.clicked.connect(self.browse_file)
            layout.addWidget(self.browse_button)
            ################################### output folder path
            self.opath=QLabel("Output folder path:",self)
            layout.addWidget(self.opath)   
            self.output_path = QLineEdit(self)
            self.output_path.setPlaceholderText("Select output path")
            layout.addWidget(self.output_path)
            self.browse_button2 = QPushButton("Browse", self)
            self.browse_button2.clicked.connect(self.browse_folder)
            layout.addWidget(self.browse_button2)
            ################################### output folder name
            self.oname=QLabel("Output folder name:",self)
            layout.addWidget(self.oname)            
            self.output_name = QLineEdit(self)
            self.output_name.setPlaceholderText("Output folder name")
            layout.addWidget(self.output_name)
            ################################### Done
            self.generate_button = QPushButton("Done", self)
            self.generate_button.clicked.connect(self.done_clicked)
            layout.addWidget(self.generate_button)
            self.setLayout(layout)
            self.setWindowTitle("X-SPAN Analysis")
        
        def browse_file(self):
            file_name, _ = QFileDialog.getOpenFileName(self, "Select File")
            if file_name:
                self.file_path.setText(file_name)
        def browse_file2(self):
            file_name, _ = QFileDialog.getOpenFileName(self, "Select File")
            if file_name:
                self.uniprot_path.setText(file_name)

        def browse_folder(self):
            folder_name = QFileDialog.getExistingDirectory(self, "Select Folder")
            if folder_name:
                self.output_path.setText(folder_name)
        def done_clicked(self):
            self.close()  # Close the window when "Done" is clicked

        def closeEvent(self, event):
            if event.spontaneous(): # this checks if the event was generated by the system, which means it was a close operation.
                exit(0)
            else: # if it was a system generated event, it means the user clicked the x
                event.accept() # Accept the close event.
    app = QApplication(sys.argv)
    window = FileNameGenerator()
    window.show()
    msg_box = show_message("""
       _  __     _____ ____  ___    _   __
      | |/ /    / ___// __ \/   |  / | / /
      |   /_____\__ \/ /_/ / /| | /  |/ / 
     /   /_____/__/ / ____/ ___ |/ /|  /  
    /_/|_|    /____/_/   /_/  |_/_/ |_/   
"""
+"\nThank you for using X-SPAN,\nthis software reads .zhrm, .mzid and .xlsx files.\n\nIf a .zhrm file is provided, it is automatically converted to .xlsx format.\n\nIf a .mzid file is provided, it is automatically converted to .xlsx format, but only if uniprotIDs are present in the source file.\n\nIf a .xlsx file is provided, the expected format is a three column file:\n \n PDB_ID (uniprot) | Residue1 | Residue2\n----------------------------------\n                  |          |\n                  |          |\n                  |          |\n")



    app.exec_()
    
    
    return window.file_path.text(), window.output_path.text(),window.output_name.text()


def process_excel(input_file_path, output_dir,output_dir_name):
    # Check if the input file exists
    if not os.path.isfile(input_file_path):
        print("Error: Input file not found. Please check the path and try again.")
        return
    
    # Check if the output directory exists
    if not os.path.isdir(output_dir):
        print("Error: Output directory not found. Please check the path and try again.")
        return
    
    # Get the base name of the input file (without extension)
    base_name = os.path.splitext(os.path.basename(input_file_path))[0]
    
    # Create a subdirectory in the specified output directory
    analyses_dir = os.path.join(output_dir, output_dir_name)
    os.makedirs(analyses_dir, exist_ok=True)
    
    # Read the Excel file
    df = pd.read_excel(input_file_path)
    
    # Sort the PDB_ID column alphabetically
    df_sorted = df.sort_values(by='PDB_ID').reset_index(drop=True)
    
    # Remove redundancy and save unique PDB_IDs in a .txt file
    unique_proteins = df_sorted['PDB_ID'].drop_duplicates()
    txt_file_path = os.path.join(analyses_dir, f"{base_name}_proteins.txt")
    unique_proteins.to_csv(txt_file_path, index=False, header=False)
    
    # Add suffixes _1, _2, etc., for duplicate PDB_IDs
    pdb_counts = {}
    new_pdb_ids = []
    for pdb in df_sorted['PDB_ID']:
        if pdb not in pdb_counts:
            pdb_counts[pdb] = 1
        else:
            pdb_counts[pdb] += 1
        new_pdb_ids.append(f"{pdb}_{pdb_counts[pdb]}")
    
    # Add the new IDs to the DataFrame
    df_sorted['PDB_ID'] = new_pdb_ids
    
    # Save the modified DataFrame in a new Excel file
    excel_file_path = os.path.join(analyses_dir, f"{base_name}_XL.xlsx")
    df_sorted.to_excel(excel_file_path, index=False)
    
    print(f"Processing completed. Files created in:\n{analyses_dir}")
    print(f"- Unique proteins: {txt_file_path}\n- Modified Excel: {excel_file_path}")
    return base_name


# Function to download a PDB file from AlphaFold
def download_pdb(uniprot_id):
    # Create the download URL for the PDB file
    url = f"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v4.pdb"
    
    # Specify the save path
    output_path = os.path.join(output_dir, f"{uniprot_id}.pdb")
    
    # Download the file
    response = requests.get(url)
    if response.status_code == 200:
        with open(output_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded: {uniprot_id}")
    else:
        print(f"Error downloading: {uniprot_id} - Status Code: {response.status_code}")



# Main function to analyze all PDB files in a folder
def analyze_pdb_folder(input_folder):
    # Specify the output file in the same folder as the input
    output_file = os.path.join(input_folder, "output.txt")
    
    with pymol2.PyMOL() as pymol:
        pymol.cmd.reinitialize()  # Reinitialize the PyMOL environment

        # Prepare the output file
        with open(output_file, 'w') as f:
            f.write("PDB_ID\tResidue_ID\tSecondary_Structure\tpLDDT_Score\n")

            # Iterate through all .pdb files in the folder
            for filename in os.listdir(input_folder):
                if filename.endswith(".pdb"):
                    pdb_path = os.path.join(input_folder, filename)
                    print(f"Processing: {filename}")

                    # Load the PDB file
                    pymol.cmd.load(pdb_path)

                    # Initialize dictionaries to store information
                    secondary_structure = {}
                    pLDDT_scores = {}

                    # Retrieve information from the loaded structure
                    pymol.cmd.iterate("n. CA", "secondary_structure[(model, resi)] = ss", space={"secondary_structure": secondary_structure})
                    pymol.cmd.iterate("n. CA", "pLDDT_scores[(model, resi)] = b", space={"pLDDT_scores": pLDDT_scores})

                    # Group residues by protein and sort them by ID
                    residues_by_protein = {}
                    for (model, resi) in secondary_structure.keys():
                        if model not in residues_by_protein:
                            residues_by_protein[model] = []
                        residues_by_protein[model].append(
                            (resi, secondary_structure[(model, resi)], pLDDT_scores.get((model, resi), ""))
                        )
                    for model in residues_by_protein:
                        residues_by_protein[model].sort(key=lambda x: int(x[0]))  # Sort by Residue ID

                    # Write information to the output file
                    for model in sorted(residues_by_protein.keys()):
                        for (resi, ss, pLDDT) in residues_by_protein[model]:
                            pdb_id = f"{model}_{resi}"  # Concatenate model name and residue ID
                            f.write(f"{pdb_id}\t{resi}\t{ss}\t{float(pLDDT):.2f}\n")

                    # Clear the PyMOL session for the next file
                    pymol.cmd.delete("all")

    print(f"Information saved in the file {output_file}")

# Step 4: Define analysis functions
def check_continuous_motif(row, structure_type, min_score=80):
    start, end = row['Residue1'], row['Residue2']
    if start == -1 or end == -1:
        return 'No continuous motif'
    
    pdb_data = pdb_groups.get(row['PDB_ID'])
    if pdb_data is None:
        return 'No continuous motif'
    
    subset = pdb_data[
        (pdb_data['Residue_ID'].between(start, end)) &
        (pdb_data['Secondary_Structure'] == structure_type)
    ]
    
    if structure_type in ['H', 'S']:
        subset = subset[subset['pLDDT_Score'] >= min_score]
    
    return f'{structure_type} ({start}, {end})' if len(subset) == end - start + 1 else 'No continuous motif'

def check_combined_motif(row, types, min_score=80):
    start, end = row['Residue1'], row['Residue2']
    if start == -1 or end == -1:
        return 'No continuous motif'
    
    pdb_data = pdb_groups.get(row['PDB_ID'])
    if pdb_data is None:
        return 'No continuous motif'
    
    subsets = [
        pdb_data[
            (pdb_data['Residue_ID'].between(start, end)) &
            (pdb_data['Secondary_Structure'] == t) &
            ((pdb_data['pLDDT_Score'] >= min_score) if t in ['H', 'S'] else True)
        ]
        for t in types
    ]
    
    if all(len(s) > 0 for s in subsets):
        return f'{"-".join(types)} ({start}, {end})'
    return 'No continuous motif'

######################################################################## FILE PARSER FUNCTIONS


def extract_uniprot(protein_entry):
    """Extracts the uniprotID from  sp|ID| -like string."""
    match = re.search(r'sp\|(.+?)\|', str(protein_entry))
    return match.group(1) if match else None

def clean_linkage_value(value):
    """Skimmes best linkage column in order to obtain just the number"""
    if str(value).strip() == "0":
        return None  
    match = re.search(r'\d+', str(value))
    return int(match.group()) if match else None

def process_merox_file(zhrm_path):
    """Processis a .zhrm file, estracts Result.csv and reurns a DataFrame and the number of found looplinks"""
    with zipfile.ZipFile(zhrm_path, 'r') as z:
        if 'Result.csv' not in z.namelist():
            print(f"File {zhrm_path} does not contain Result.csv, skipped.")
            return pd.DataFrame(), 0
        
        with z.open('Result.csv') as f:
            columns = ['Score', 'm/z', 'Charge', 'M+H+', 'Calculated Mass', 'Deviation in ppm', 'Peptide 1', 'Protein 1',
                       'From_1', 'To_1', 'Peptide2', 'Protein 2', 'From_2', 'To_2', 'Scan number', 'is Selected in Table',
                       'Candidate identifier', 'Folder Number', 'Retention time in sec', 'miscellaneous',
                       'best linkage position peptide 1', 'best linkage position peptide 2', 'All linkage positions',
                       'Spectrum UUID', 'local False discovery rate', 'shortest distance in pdb', 'Light/Heavy(1/2)',
                       'pepScore1', 'pepScore2', 'xLinkScore', 'resultId', 'MS1intensity', 'finalScoreComponent']
            
            df = pd.read_csv(f, delimiter=';', names=columns, skiprows=1, encoding='latin-1', low_memory=False, on_bad_lines='skip')
            print(f"File: {zhrm_path} - Righe lette: {len(df)}")
            df['PDB_ID'] = df['Protein 1'].apply(extract_uniprot)
            
           
            num_intrapeptidal = 0
            
            
            def calculate_residues(row):
                """Calculates Residue1 and Residue2 with looplink handling."""
                from_1 = pd.to_numeric(row['From_1'], errors='coerce')
                from_2 = pd.to_numeric(row['From_2'], errors='coerce')
                
                linkage_1 = clean_linkage_value(row['best linkage position peptide 1'])
                linkage_2 = clean_linkage_value(row['best linkage position peptide 2'])
                
                if pd.isna(from_1) or linkage_1 is None or (linkage_2 is None and row['Protein 2'] != 'intrapeptidal'):
                    return None, None 

                if row['Protein 2'] == 'intrapeptidal':
                    
                    res1 = from_1 + linkage_1 - 1
                    res2 = from_1 + linkage_2 - 1
                    nonlocal num_intrapeptidal
                    num_intrapeptidal += 1  
                else:
                    
                    if pd.isna(from_2):
                        return None, None  
                    res1 = from_1 + linkage_1 - 1
                    res2 = from_2 + linkage_2 - 1
                
                return int(res1), int(res2)
            
            df[['Residue1', 'Residue2']] = df.apply(lambda row: pd.Series(calculate_residues(row)), axis=1)
            
           
            df = df.dropna(subset=['Residue1', 'Residue2'])
            
            
            df['Residue1'] = df['Residue1'].astype(int)
            df['Residue2'] = df['Residue2'].astype(int)
            
            
            df[['Residue1', 'Residue2']] = df[['Residue1', 'Residue2']].apply(lambda x: sorted(x.values), axis=1, result_type='expand')
            
            
            df_filtered = df[df['PDB_ID'].notnull()]
            
           
            df_filtered = df_filtered[abs(df_filtered['Residue2'] - df_filtered['Residue1']) <= 20]
            
            
            df_filtered = df_filtered[(df_filtered['Residue1'] > 0) & (df_filtered['Residue2'] > 0)]
            
           
            df_filtered = df_filtered.drop_duplicates()
            
          
            print(f"Looplink (intrapeptidal) inclusi nel file {zhrm_path}: {num_intrapeptidal}")
            
            return df_filtered[['PDB_ID', 'Residue1', 'Residue2']], num_intrapeptidal

def merge_merox_results(input_folder,output_file):
    """Processes all .zhrm in the specified folder and saves a single file in Excel format"""
    merged_df = pd.DataFrame()
    total_intrapeptidal = 0  # Contatore globale per tutti i looplink
    
    for file_name in os.listdir(input_folder):
        if file_name.endswith('.zhrm'):
            file_path = os.path.join(input_folder, file_name)
            print(f"Processing: {file_name}")
            df, num_intrapeptidal = process_merox_file(file_path)
            merged_df = pd.concat([merged_df, df], ignore_index=True)
            total_intrapeptidal += num_intrapeptidal
    
    
    merged_df = merged_df.drop_duplicates()
    
   
    merged_df.to_excel(output_file, index=False)
    
    print(f"Merging complete: {output_file}")
    print(f"Total included looplinks (intrapeptidal): {total_intrapeptidal}")


def find_extension(input_file_path):
    elem=input_file_path.split(".")
    ext=elem[-1]
    base_name_a=input_file_path.replace("."+ext,"")
    elem=base_name_a.split("/")
    base_name=elem[-1]
    return ext,base_name

##########################################################################
input_file_path,output_dir_path,output_dir_name=GUI()
##########################################################################MODIFY THE FILES IF NEEDED
ext,base_name=find_extension(input_file_path)
if ext=="xlsx":
    ext=ext
elif ext=="zhrm":
    input_folder=os.path.join(output_dir_path,output_dir_name)
    output_new_dir=os.path.join(output_dir_path,output_dir_name)
    if not os.path.exists(output_new_dir):
        os.makedirs(output_new_dir)
    output_xlsx_filtered = os.path.join(str(output_new_dir), f"{base_name}.xlsx")
    output_file=output_xlsx_filtered
    source_dir_path=os.path.abspath(os.path.dirname(input_file_path))
    merge_merox_results(source_dir_path,output_file)
    base_path = output_dir_path
    mzid_file = os.path.join(input_file_path)
    output_xlsx_filtered = os.path.join(output_new_dir, f"{base_name}.xlsx")
    input_file_path=output_xlsx_filtered
    output_dir_path=output_dir_path
    output_dir_name=output_dir_name
elif ext=="mzid":
    # Parse the mzIdentML file
    output_new_dir=os.path.join(output_dir_path,output_dir_name)
    if not os.path.exists(output_new_dir):
        os.makedirs(output_new_dir)
    output_xlsx_filtered = os.path.join(output_new_dir, f"{base_name}.xlsx")
    mzid_file=input_file_path
    tree = ET.parse(mzid_file)
    root = tree.getroot()

    # mzIdentML namespace
    ns = {'mzid': 'http://psidev.info/psi/pi/mzIdentML/1.3'}

    # List to store only intra-protein or loop link crosslinks
    filtered_crosslinks = []

    # Iterate over PeptideEvidence elements to extract protein and crosslink sites
    for peptide_evidence in root.findall(".//mzid:PeptideEvidence", ns):
        protein_name = peptide_evidence.get("dBSequence_ref")  # Protein name
        peptide_ref = peptide_evidence.get("peptide_ref")  # Corresponding peptide

        # Get the peptide sequence to locate crosslink sites
        peptide_element = root.find(f".//mzid:Peptide[@id='{peptide_ref}']", ns)
        if peptide_element is not None:
            modifications = peptide_element.findall("mzid:Modification", ns)
            
            # Extract crosslink sites and verify type
            crosslink_sites = [int(mod.get("location")) for mod in modifications if mod.get("location")]
            
            # Check if the crosslink is intra-protein or loop link (same peptide)
            if len(crosslink_sites) == 2:  # Intra-protein or loop link should have exactly 2 sites
                residue1, residue2 = sorted(crosslink_sites)  # Ensure Residue1 < Residue2
                
                # Only add if the difference between Residue1 and Residue2 is < 20
                if abs(residue2 - residue1) < 20:
                    filtered_crosslinks.append([protein_name, residue1, residue2])

    # Create DataFrame with only intra-protein/loop link crosslinks
    if filtered_crosslinks:
        df_filtered_crosslinks = pd.DataFrame(filtered_crosslinks, columns=["PDB_ID", "Residue1", "Residue2"])
        df_filtered_crosslinks.to_excel(output_xlsx_filtered, index=False)
        print(f"File saved as {output_xlsx_filtered}")
    else:
        print("No valid crosslinks found matching the criteria.")
        exit(0)
else:
    print("Unsupported file extension")
    exit(0)

############################################################################
base_name=process_excel(input_file_path, output_dir_path,output_dir_name)
output_dir = os.path.join(output_dir_path, output_dir_name)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
# Leggi gli ID UniProt da un file txt

ids_file_name=os.path.join(output_dir,f"{base_name}_proteins.txt")
with open(ids_file_name, 'r') as f:
    uniprot_ids = [line.strip() for line in f.readlines()]

# Scarica tutti i file nella lista
for uniprot_id in uniprot_ids:
    download_pdb(uniprot_id)


# Code execution
input_folder = output_dir # Replace with the folder path containing the PDB files
analyze_pdb_folder(input_folder)

# Initialize tqdm for pandas
tqdm.pandas()

# Start the timer
start_time = time.time()

# Step 1: Load data
print("ðŸ“¥ Loading input files...")

pdb_df = pd.read_csv(
    input_folder+'/output.txt',
    delimiter='\t'
)

xl_df = pd.read_excel(output_dir+f"/{base_name}_XL.xlsx")

# Merge data on 'PDB_ID'
merged_df = pd.merge(pdb_df, xl_df, on='PDB_ID', how='left')
merged_df["PDB_ID"] = merged_df["PDB_ID"].str.split("_").str[0]

# Work directly on the merged dataframe without saving intermediate file
df = merged_df.copy()

# Step 2: Pre-processing
default_value = -1
df['Residue1'] = df['Residue1'].fillna(default_value).astype(int)
df['Residue2'] = df['Residue2'].fillna(default_value).astype(int)

# Step 3: Pre-filter by PDB_ID to improve performance
print("ðŸ“Š Indexing by PDB_ID...")
pdb_groups = {pdb_id: group for pdb_id, group in df.groupby('PDB_ID')}


# Step 5: Apply functions with progress bar
print("ðŸ” Analyzing structural motifs...")

df['Continuous Helix'] = df.progress_apply(lambda row: check_continuous_motif(row, 'H'), axis=1)
df['Continuous Sheet'] = df.progress_apply(lambda row: check_continuous_motif(row, 'S'), axis=1)
df['Continuous Loop'] = df.progress_apply(lambda row: check_continuous_motif(row, 'L'), axis=1)

df['Continuous Helix-Loop'] = df.progress_apply(lambda row: check_combined_motif(row, ['H', 'L']), axis=1)
df['Continuous Sheet-Loop'] = df.progress_apply(lambda row: check_combined_motif(row, ['S', 'L']), axis=1)
df['Continuous Helix-Loop-Sheet'] = df.progress_apply(lambda row: check_combined_motif(row, ['H', 'L', 'S']), axis=1)

# Step 6: Save output
df.to_csv(output_dir+f"/{base_name}_results.csv", index=False)
print("âœ… Analysis completed. File saved!")

# Print total elapsed time
elapsed = time.time() - start_time
print(f"â±ï¸ Total time: {elapsed:.2f} seconds")




# File path
file_path = output_dir+f"/{base_name}_results.csv"

# Load the CSV file
df = pd.read_csv(file_path)

# Select columns G to M (6:12 index)
columns_to_check = df.columns[6:12]

# ---------------- filtered_df ----------------
# Keep rows where not all values in columns G-M are "No continuous motif"
filtered_df = df[~(df[columns_to_check] == "No continuous motif").all(axis=1)]

# ---------------- filtered1_df ----------------
# Keep rows where at least one value in columns G-M is different from "No continuous motif"
filtered1_df = df[(df[columns_to_check] != "No continuous motif").any(axis=1)]

# ---------------- filtered2_df ----------------
# Keep only columns G-M
filtered2_df = df[columns_to_check]

# Remove rows where all values are "No continuous motif"
filtered2_df = filtered2_df[~(filtered2_df == "No continuous motif").all(axis=1)]

# ---------------- filtered3_df ----------------
# Function to extract AA distance from string values like "H (123, 129)"
def extract_distance(value):
    try:
        match = re.search(r'\((\d+),\s*(\d+)\)', str(value))
        if match:
            start, end = map(int, match.groups())
            return abs(end - start)
    except:
        pass
    return None

# Apply the function to each cell in filtered2_df
filtered3_df = filtered2_df.applymap(extract_distance)

# ---------------- counts_df ----------------
# Count occurrences of each AA distance from 1 to 20
counts_df = pd.DataFrame()

for column in filtered3_df.columns:
    counts = filtered3_df[column].value_counts().reindex(range(1, 21), fill_value=0)
    counts_df = pd.concat([
        counts_df,
        pd.DataFrame({'AA Distance': counts.index, f'Count_{column}': counts.values})
    ], axis=1) if not counts_df.empty else pd.DataFrame({'AA Distance': counts.index, f'Count_{column}': counts.values})

# ---------------- Save to CSV ----------------
filtered_df.to_csv(output_dir+'/filtered.csv', index=False)
filtered1_df.to_csv(output_dir+'/filtered1.csv', index=False)
filtered2_df.to_csv(output_dir+'/filtered2.csv', index=False)
filtered3_df.to_csv(output_dir+'/filtered3.csv', index=False)
counts_df.to_csv(output_dir+'/counts.csv', index=False)

print("âœ… All CSV files created and saved successfully.")


# CSV file path
file_path = output_dir+'/counts.csv'

# Extract the directory of the input file
output_directory = os.path.dirname(file_path)

# Load the CSV file
counts_df = pd.read_csv(file_path)

# Identify columns to create graphs in pairs
columns = counts_df.columns
num_columns = len(columns)

# Create the plots
for i in range(0, num_columns, 2):
    if i + 1 < num_columns:
        x_column = columns[i]  # X-axis column
        y_column = columns[i + 1]  # Y-axis column

        # Create the plot
        plt.figure(figsize=(8, 6))
        plt.plot(counts_df[x_column], counts_df[y_column], marker='o', linestyle='-', color='b')
        plt.xlabel(x_column)
        plt.ylabel(y_column)
        plt.title(f'Plot: {y_column}')
        plt.grid(True)

        # Set all ticks on the X-axis
        plt.xticks(counts_df[x_column], rotation=45)  # Show all ticks with a 45Â° rotation

        # Output file name
        output_file = os.path.join(output_directory, f'{y_column.lower()}.png')  # Full path
        
        # Save the plot
        plt.savefig(output_file, dpi=300, bbox_inches='tight')  # `bbox_inches='tight'` prevents label clipping
        plt.close()

print(f"Plots successfully created and saved in the directory: {output_directory}")

# CSV file path
file_path = output_dir+'/counts.csv'

# Extract the directory of the input file
output_directory = os.path.dirname(file_path)

# Load the CSV file
counts_df = pd.read_csv(file_path)

# Create a copy of the DataFrame for modifications
updated_counts_df = counts_df.copy()

# Iterate through columns in pairs
new_columns = []  # To maintain the order of new columns
for i in range(0, len(counts_df.columns), 2):
    x_col = counts_df.columns[i]
    count_col = counts_df.columns[i + 1]

    # Calculate the total sum of the "Count" column
    total_count = counts_df[count_col].sum()

    # Calculate the percentage frequency
    frequency_col_name = f"Frequency({x_col.split()[-1]}), %"
    updated_counts_df[frequency_col_name] = (counts_df[count_col] / total_count) * 100

    # Update the order of columns
    new_columns.extend([x_col, count_col, frequency_col_name])

    # ------------------- Create the Plot -------------------
    plt.figure(figsize=(8, 6))
    plt.plot(counts_df[x_col], updated_counts_df[frequency_col_name], marker='o', linestyle='-', color='b', label='Frequency %')
    plt.xlabel(x_col, fontsize=12)
    plt.ylabel(frequency_col_name, fontsize=12)
    plt.title(f"AA Distance vs {frequency_col_name}", fontsize=14)
    plt.grid(True)
    plt.legend()
    
    # Save the plot as PNG
    chart_name = f"{frequency_col_name.replace(' ', '_').replace(',', '').replace('%', '')}.png"
    chart_path = os.path.join(output_directory, chart_name)
    plt.savefig(chart_path, dpi=300)
    plt.close()
    print(f"Plot saved: {chart_path}")

# Reorder columns in the updated DataFrame
updated_counts_df = updated_counts_df[new_columns]

# Save the updated DataFrame to CSV
updated_counts_df.to_csv(output_dir+'/counts_updated.csv', index=False)

print("File 'counts_updated.csv' successfully created and saved!")


# In[1]:


print("Work Done ;)")


# In[ ]:




